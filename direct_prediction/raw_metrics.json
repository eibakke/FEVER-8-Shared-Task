{
  "agreement": {
    "overall_agreement": 0.312,
    "agreement_by_direct_label": {
      "Conflicting Evidence/Cherrypicking": {
        "count": 33,
        "mean": 0.06060606060606061
      },
      "Not Enough Evidence": {
        "count": 235,
        "mean": 0.03404255319148936
      },
      "Refuted": {
        "count": 133,
        "mean": 0.8120300751879699
      },
      "Supported": {
        "count": 99,
        "mean": 0.3838383838383838
      }
    },
    "agreement_by_baseline_label": {
      "Conflicting Evidence/Cherrypicking": {
        "count": 12,
        "mean": 0.16666666666666666
      },
      "Not Enough Evidence": {
        "count": 12,
        "mean": 0.6666666666666666
      },
      "Refuted": {
        "count": 334,
        "mean": 0.32335329341317365
      },
      "Supported": {
        "count": 142,
        "mean": 0.2676056338028169
      }
    },
    "transition_matrix": {
      "Conflicting Evidence/Cherrypicking": {
        "Conflicting Evidence/Cherrypicking": 0.061,
        "Not Enough Evidence": 0.061,
        "Refuted": 0.667,
        "Supported": 0.212
      },
      "Not Enough Evidence": {
        "Conflicting Evidence/Cherrypicking": 0.021,
        "Not Enough Evidence": 0.034,
        "Refuted": 0.621,
        "Supported": 0.323
      },
      "Refuted": {
        "Conflicting Evidence/Cherrypicking": 0.015,
        "Not Enough Evidence": 0.015,
        "Refuted": 0.812,
        "Supported": 0.158
      },
      "Supported": {
        "Conflicting Evidence/Cherrypicking": 0.03,
        "Not Enough Evidence": 0.0,
        "Refuted": 0.586,
        "Supported": 0.384
      }
    },
    "flip_Conflicting Evidence/Cherrypicking_to_Not Enough Evidence": {
      "count": 2,
      "percentage_of_changes": 0.58
    },
    "flip_Conflicting Evidence/Cherrypicking_to_Refuted": {
      "count": 22,
      "percentage_of_changes": 6.4
    },
    "flip_Conflicting Evidence/Cherrypicking_to_Supported": {
      "count": 7,
      "percentage_of_changes": 2.03
    },
    "flip_Not Enough Evidence_to_Conflicting Evidence/Cherrypicking": {
      "count": 5,
      "percentage_of_changes": 1.45
    },
    "flip_Not Enough Evidence_to_Refuted": {
      "count": 146,
      "percentage_of_changes": 42.44
    },
    "flip_Not Enough Evidence_to_Supported": {
      "count": 76,
      "percentage_of_changes": 22.09
    },
    "flip_Refuted_to_Conflicting Evidence/Cherrypicking": {
      "count": 2,
      "percentage_of_changes": 0.58
    },
    "flip_Refuted_to_Not Enough Evidence": {
      "count": 2,
      "percentage_of_changes": 0.58
    },
    "flip_Refuted_to_Supported": {
      "count": 21,
      "percentage_of_changes": 6.1
    },
    "flip_Supported_to_Conflicting Evidence/Cherrypicking": {
      "count": 3,
      "percentage_of_changes": 0.87
    },
    "flip_Supported_to_Refuted": {
      "count": 58,
      "percentage_of_changes": 16.86
    },
    "overall_flip_rate": 0.688
  },
  "correctness": {
    "direct_accuracy": 0.32,
    "baseline_accuracy": 0.692,
    "accuracy_delta": 0.37199999999999994,
    "correction_opportunities": 340,
    "corrections_made": 227,
    "correction_rate": 0.6676470588235294,
    "error_opportunities": 160,
    "errors_introduced": 41,
    "error_introduction_rate": 0.25625,
    "net_corrections": 186,
    "correctness_by_label": {
      "gold_label": {
        "0": "Conflicting Evidence/Cherrypicking",
        "1": "Not Enough Evidence",
        "2": "Refuted",
        "3": "Supported"
      },
      "direct_correct": {
        "0": 0.07894736842105263,
        "1": 0.6857142857142857,
        "2": 0.32131147540983607,
        "3": 0.28688524590163933
      },
      "baseline_correct": {
        "0": 0.05263157894736842,
        "1": 0.05714285714285714,
        "2": 0.8491803278688524,
        "3": 0.680327868852459
      }
    },
    "accuracy_when_agree": {
      "count": 156,
      "direct_accuracy": 0.7628205128205128,
      "baseline_accuracy": 0.7628205128205128
    },
    "accuracy_when_disagree": {
      "count": 344,
      "direct_accuracy": 0.11918604651162791,
      "baseline_accuracy": 0.6598837209302325
    }
  },
  "evidence": {
    "avg_direct_evidence": 2.976,
    "avg_baseline_evidence": 10.0,
    "evidence_when_agree": {
      "count": 156,
      "avg_baseline_evidence": 10.0
    },
    "evidence_when_disagree": {
      "count": 344,
      "avg_baseline_evidence": 10.0
    },
    "correlation_evidence_disagreement": NaN,
    "evidence_by_transition": {
      "direct_label": {
        "0": "Conflicting Evidence/Cherrypicking",
        "1": "Conflicting Evidence/Cherrypicking",
        "2": "Conflicting Evidence/Cherrypicking",
        "3": "Conflicting Evidence/Cherrypicking",
        "4": "Not Enough Evidence",
        "5": "Not Enough Evidence",
        "6": "Not Enough Evidence",
        "7": "Not Enough Evidence",
        "8": "Refuted",
        "9": "Refuted",
        "10": "Refuted",
        "11": "Refuted",
        "12": "Supported",
        "13": "Supported",
        "14": "Supported"
      },
      "baseline_label": {
        "0": "Conflicting Evidence/Cherrypicking",
        "1": "Not Enough Evidence",
        "2": "Refuted",
        "3": "Supported",
        "4": "Conflicting Evidence/Cherrypicking",
        "5": "Not Enough Evidence",
        "6": "Refuted",
        "7": "Supported",
        "8": "Conflicting Evidence/Cherrypicking",
        "9": "Not Enough Evidence",
        "10": "Refuted",
        "11": "Supported",
        "12": "Conflicting Evidence/Cherrypicking",
        "13": "Refuted",
        "14": "Supported"
      },
      "baseline_evidence_count": {
        "0": 10.0,
        "1": 10.0,
        "2": 10.0,
        "3": 10.0,
        "4": 10.0,
        "5": 10.0,
        "6": 10.0,
        "7": 10.0,
        "8": 10.0,
        "9": 10.0,
        "10": 10.0,
        "11": 10.0,
        "12": 10.0,
        "13": 10.0,
        "14": 10.0
      },
      "claim_id": {
        "0": 2,
        "1": 2,
        "2": 22,
        "3": 7,
        "4": 5,
        "5": 8,
        "6": 146,
        "7": 76,
        "8": 2,
        "9": 2,
        "10": 108,
        "11": 21,
        "12": 3,
        "13": 58,
        "14": 38
      }
    }
  },
  "distribution": {
    "direct_label_counts": {
      "Not Enough Evidence": 235,
      "Refuted": 133,
      "Supported": 99,
      "Conflicting Evidence/Cherrypicking": 33
    },
    "baseline_label_counts": {
      "Refuted": 334,
      "Supported": 142,
      "Not Enough Evidence": 12,
      "Conflicting Evidence/Cherrypicking": 12
    },
    "direct_label_percentages": {
      "Not Enough Evidence": 0.47,
      "Refuted": 0.266,
      "Supported": 0.198,
      "Conflicting Evidence/Cherrypicking": 0.066
    },
    "baseline_label_percentages": {
      "Refuted": 0.668,
      "Supported": 0.284,
      "Not Enough Evidence": 0.024,
      "Conflicting Evidence/Cherrypicking": 0.024
    },
    "label_shifts": {
      "Conflicting Evidence/Cherrypicking": {
        "absolute_shift": -0.042,
        "relative_shift": -0.6363636363636364
      },
      "Not Enough Evidence": {
        "absolute_shift": -0.44599999999999995,
        "relative_shift": -0.948936170212766
      },
      "Refuted": {
        "absolute_shift": 0.402,
        "relative_shift": 1.511278195488722
      },
      "Supported": {
        "absolute_shift": 0.08599999999999997,
        "relative_shift": 0.43434343434343425
      }
    }
  },
  "justification": {
    "mean_justification_similarity": 0.5885725021362305,
    "median_justification_similarity": 0.6166383028030396,
    "min_justification_similarity": -0.00016057491302490234,
    "max_justification_similarity": 0.9114428758621216,
    "mean_similarity_when_agree": 0.6063165739178658,
    "mean_similarity_when_disagree": 0.5802223436972674
  }
}